{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer\n",
    "- Only stem words per line in a file.\n",
    "- It cannot stem if the word is still a part of a sentence.\n",
    "- Instead of using morphological analyzer, this stemmer searches  \n",
    "for affixes attached in a word using string matching which may  \n",
    "result to over-stemming of under-stemmign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of affixes that are used in stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infix = ['um', 'in']\n",
    "prefix = [\n",
    "          'makapang', 'nakapang', 'nakapan', 'makapan', 'nakapam',\n",
    "          'makapam', 'nakapag', 'makapag', 'nagpati', 'magpati', 'nagpaka',\n",
    "          'magpaka', 'magpapa', 'nagpapa', 'nagkaka', 'magkaka', 'ipakipa',\n",
    "          'ikapang',  'naipag', 'mangag', 'nangag', 'kasing', 'ipang', 'pinag',\n",
    "          'papag', 'mapag', 'kapag', 'napag', 'ipaki', 'magka', 'magsi',\n",
    "          'nagka', 'magma', 'nagma', 'magpa',  'nagpa', 'maipa', 'naipa',\n",
    "          'kasim', 'pagpa', 'paka', 'paki', 'pang', 'ipag', 'mang', 'nang',\n",
    "          'maka', 'naka', 'mapa', 'napa', 'ika', 'pag', 'isa', 'ipa', 'mai',\n",
    "          'nai', 'mag', 'nag', 'man', 'nan', 'mam', 'nam', 'ma', 'na', 'ka',\n",
    "          'pa'\n",
    "          ]\n",
    "suffix = ['han', 'hin', 'an', 'in', 'ng']\n",
    "vowel = ['a', 'e', 'i', 'o', 'u']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------\n",
    "#### The Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    input = open(os.path.abspath('files/labelled_data.csv'))\n",
    "    output = open(os.path.abspath('files/dataset_pos.in'), 'w+')\n",
    "\n",
    "    # Rearrange the data into vertical sentences.\n",
    "    # Each sentence ending with a questions mark.\n",
    "    format_data(input, output)\n",
    "\n",
    "    # Reads the output file of format_data().\n",
    "    wordfile = open(os.path.abspath('files/dataset_pos.in'))\n",
    "    # wordfile = open(os.path.abspath('preprocessing/test_stemmer.txt'))\n",
    "\n",
    "    stemmed_data = list()\n",
    "    \n",
    "    # And each word in the file is stemmed.\n",
    "    for w in wordfile.readlines():\n",
    "        if len(w.strip()) > 0:\n",
    "            infix_dict = {'word': \"\", 'infix': \"_\"}\n",
    "            pref_dict = {'word': \"\", 'prefix': \"_\", 'has_pref': False}\n",
    "            redup_dict = {'word': \"\", 'redup': \"_\"}\n",
    "            suf_dict = {'word': \"\", 'suffix': \"_\"}\n",
    "            morphology = {\n",
    "                        'word': \"\", 'root': \"\", 'prefix': \"_\", 'infix': \"_\",\n",
    "                        'suffix': \"_\", 'redup': \"_\"\n",
    "                        }\n",
    "            root = w\n",
    "            temp_word = w\n",
    "            if len(w) > 5:\n",
    "                infix_dict = strip_infix(w.lower())  # remove any infix first\n",
    "                temp_word = infix_dict['word']\n",
    "\n",
    "            # usually root words are 4-5 characters in length\n",
    "            if len(temp_word.strip()) >= 5:\n",
    "                # try to check prefixes, suffixes and reduplications\n",
    "                pref_dict = strip_prefix(temp_word)\n",
    "                temp_word = pref_dict['word']\n",
    "                suf_dict = strip_suffix(temp_word)\n",
    "                temp_word = suf_dict['word']\n",
    "                redup_dict = check_reduplication(temp_word)\n",
    "                temp_word = redup_dict['word']\n",
    "\n",
    "            root = temp_word\n",
    "\n",
    "            morphology['word'] = w.strip()\n",
    "            morphology['root'] = root.strip()\n",
    "            morphology['prefix'] = pref_dict['prefix']\n",
    "            morphology['infix'] = infix_dict['infix']\n",
    "            morphology['suffix'] = suf_dict['suffix']\n",
    "            morphology['redup'] = redup_dict['redup']\n",
    "\n",
    "            # Append the stemmed words in the list of stemmed data.\n",
    "            # print(w.strip(), root.strip())\n",
    "            stemmed_data.append(morphology)\n",
    "\n",
    "    # Write the stemmed data into a file as an output.\n",
    "    write_to_file(stemmed_data)\n",
    "    # print(stemmed_data)\n",
    "\n",
    "    wordfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # count_qmark(open(os.path.abspath('files/dataset_pos.in')))\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------\n",
    "\n",
    "Methods used for the extraction of the affixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_infix(word):\n",
    "    dictionary = {'word': \"\", 'infix': \"_\"}\n",
    "\n",
    "    for inf in infix:\n",
    "        if (inf in word):\n",
    "            start_pos = re.search(inf, word).start()\n",
    "            end_pos = re.search(inf, word).end()\n",
    "\n",
    "            # checks the position of infix; removes infix only if in the\n",
    "            # middle,start position, or word length > 3\n",
    "            if (start_pos >= 0 and end_pos < len(word) and len(word) > 4):\n",
    "                # if word ! in dictionary\n",
    "                word = re.sub(inf, '', word, 1)\n",
    "\n",
    "            dictionary['infix'] = inf\n",
    "\n",
    "    dictionary['word'] = word\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_suffix(word):\n",
    "    d = {'word': \"\", 'suffix': \"_\"}\n",
    "\n",
    "    for suf in suffix:\n",
    "        if (suf in word):\n",
    "            if(word.endswith(suf) and len(word) > 4):\n",
    "                word = re.sub(suf+'$', '', word, 1)\n",
    "                d['suffix'] = suf\n",
    "\n",
    "    d['word'] = word\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_prefix(word):\n",
    "    # temp_word,prefix_found,the_prefix\n",
    "    d = {'word': \"\", 'prefix': \"_\", 'has_pref': False}\n",
    "\n",
    "    for pre in prefix:\n",
    "        if(pre in word):\n",
    "            if(word.startswith(pre) and len(word) > 5):\n",
    "                word = re.sub(pre, '', word, 1)\n",
    "                d['prefix'] = pre\n",
    "                d['has_pref'] = True\n",
    "\n",
    "        if word.startswith(\"-\"):\n",
    "            word = word[1:]\n",
    "\n",
    "        # checks reduplication of vowel prefixes\n",
    "        for char in vowel:\n",
    "            if(word.startswith(char) and len(word) > 1):\n",
    "                if(word[0] == word[1]):\n",
    "                    word = word[1:]\n",
    "\n",
    "        # word = check_reduplication(word)\n",
    "        d['word'] = word\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_reduplication(word):\n",
    "    d = {'word': \"\", 'redup': \"_\"}\n",
    "\n",
    "    for i in range(1, 3):\n",
    "        if word[:i] == word[i:i+i]:  # python substring;\n",
    "            word = word[i:]\n",
    "            d['redup'] = word[:i]\n",
    "            break\n",
    "\n",
    "    d['word'] = word\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------\n",
    "#### Helper functions:\n",
    "\n",
    "- **format_data()** - reads the cleaned raw data line by line. It converts  \n",
    "the horizontal sentences into vertical sentences including the  \n",
    "question mark. Each sentence is separated by a _single space_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(input, output):\n",
    "    input_file = input\n",
    "    output_file = output\n",
    "    csv_f = csv.reader(input_file)\n",
    "    ctr = 0\n",
    "\n",
    "    for row in csv_f:\n",
    "        # print row\n",
    "        if ctr == 0:\n",
    "            ctr += 1\n",
    "        else:\n",
    "            for column in range(0, len(row)-1):\n",
    "                sen = row[column].split(\" \")\n",
    "                for word in sen:\n",
    "                    if '?' in word:\n",
    "                        word = word.replace('?', '')\n",
    "                        output_file.write(word)\n",
    "                        output_file.write(\"\\n\")\n",
    "                        output_file.write(\"?\")\n",
    "                        output_file.write(\"\\n\")\n",
    "                        output_file.write(\"\\n\")\n",
    "                    elif word != \"\":\n",
    "                        output_file.write(word)\n",
    "                        output_file.write(\"\\n\")\n",
    "\n",
    "    input_file.close()\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **write_to_file** - format the data into 7 columns  \n",
    "```\n",
    "WORD | ROOT | prefix | infix | suffix reduplication | POSTAG\n",
    "```\n",
    "and write in a file named `dataset_pos.in`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(stemmed_data):\n",
    "    output_file = open(os.path.abspath('files/dataset_pos.in'), 'w')\n",
    "\n",
    "    for dict in stemmed_data:\n",
    "        output_file.write(dict['word'])\n",
    "        output_file.write(\" \")\n",
    "        output_file.write(dict['root'])\n",
    "        output_file.write(\" \")\n",
    "        output_file.write(dict['prefix'])\n",
    "        output_file.write(\" \")\n",
    "        output_file.write(dict['infix'])\n",
    "        output_file.write(\" \")\n",
    "        output_file.write(dict['suffix'])\n",
    "        output_file.write(\" \")\n",
    "        output_file.write(dict['redup'])\n",
    "        output_file.write(\" \")\n",
    "        output_file.write(\"XXX\")\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "        if dict['word'] == \"?\":\n",
    "            output_file.write(\"\\n\")\n",
    "\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **count_qmark** - count and display the number of sentences in the raw data by  \n",
    "counting the number of question mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_qmark(file):\n",
    "    \"\"\"\n",
    "    count and display the number of sentences in the raw data by\n",
    "    counting the number of question mark\n",
    "    \"\"\"\n",
    "    text = file.readlines()\n",
    "    ctr = 0\n",
    "\n",
    "    for line in text:\n",
    "        if \"?\" in line:\n",
    "            ctr += 1\n",
    "\n",
    "    print(ctr)\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
